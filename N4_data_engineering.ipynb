{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè¶ Bank Customer Churn Prediction\n",
    "## Notebook 4 ‚Äî Feature Engineering & Preprocessing\n",
    "\n",
    "**Goal:** Transform the cleaned dataset into the numeric format required by scikit-learn models.\n",
    "\n",
    "Two transformations happen here:\n",
    "1. **Standardise numerical features** ‚Äî put all features on the same scale (mean=0, std=1).\n",
    "2. **Encode categorical features** ‚Äî convert text labels to numeric dummy variables.\n",
    "\n",
    "### ‚ö†Ô∏è Why SMOTE is NOT applied here\n",
    "\n",
    "A common mistake is to apply SMOTE (oversampling) to the **full dataset** before the train/test split.  \n",
    "This causes **data leakage** into the test set:\n",
    "\n",
    "```\n",
    "‚ùå WRONG ORDER (what causes 100% fake accuracy)\n",
    "   Full data ‚Üí SMOTE ‚Üí train/test split\n",
    "   Problem: synthetic samples generated from ALL data appear in the test set.\n",
    "             The model has effectively seen variations of the test samples during training.\n",
    "             Test accuracy becomes artificially inflated and meaningless.\n",
    "\n",
    "‚úÖ CORRECT ORDER (implemented in N5)\n",
    "   Full data ‚Üí train/test split ‚Üí SMOTE on X_train only ‚Üí train model ‚Üí evaluate on REAL test set\n",
    "   The test set contains only original, real customers the model has never seen.\n",
    "```\n",
    "\n",
    "This notebook saves the **imbalanced** (but scaled and encoded) data.  \n",
    "SMOTE is applied correctly in N5, **after** the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "df = pd.read_csv('df_cleaned.csv')\n",
    "print(f'Cleaned data loaded: {df.shape}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Separate Features (X) from Target (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Exited', axis=1)   # Feature matrix ‚Äî everything except the target\n",
    "y = df['Exited']                 # Target vector ‚Äî 0 or 1\n",
    "\n",
    "print(f'X shape: {X.shape}  (features)')\n",
    "print(f'y shape: {y.shape}  (target)')\n",
    "print()\n",
    "print('Feature columns:', X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Standardise Numerical Features\n",
    "\n",
    "Many ML algorithms are sensitive to the *scale* of features. Without standardisation, `Balance` (range 0‚Äì250,000) would dominate `Tenure` (range 0‚Äì10) simply due to magnitude.\n",
    "\n",
    "**StandardScaler** transforms each column to **mean = 0, std = 1**:  \n",
    "`z = (x - Œº) / œÉ`\n",
    "\n",
    "We use a **CustomScaler** that only scales selected columns. Binary flags like `HasCrCard` and `IsActiveMember` must NOT be scaled ‚Äî their 0/1 values are already meaningful.\n",
    "\n",
    "> **Critical:** We call `.fit()` on the **full X** here because in N5 we will split\n",
    "> the already-scaled data. The scaler learns mean/std from all 10,000 real customers,\n",
    "> which is the correct reference for inference on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Applies StandardScaler only to specified columns.\n",
    "    All other columns are returned unchanged.\n",
    "    Inheriting from BaseEstimator + TransformerMixin gives sklearn compatibility\n",
    "    and a free .fit_transform() method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, columns, copy=True, with_mean=True, with_std=True):\n",
    "        self.scaler    = StandardScaler(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        self.columns   = columns\n",
    "        self.with_mean = with_mean\n",
    "        self.with_std  = with_std\n",
    "        self.copy      = copy\n",
    "        self.mean_     = None\n",
    "        self.std_      = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Learn mean and std from the data (called once on training data).\"\"\"\n",
    "        self.scaler.fit(X[self.columns], y)\n",
    "        self.mean_ = np.mean(X[self.columns])\n",
    "        self.std_  = np.std(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        \"\"\"Apply scaling, preserving the original column order.\"\"\"\n",
    "        init_col_order = X.columns\n",
    "        X_scaled    = pd.DataFrame(\n",
    "            self.scaler.transform(X[self.columns]),\n",
    "            columns=self.columns,\n",
    "            index=X.index\n",
    "        )\n",
    "        X_notscaled = X.loc[:, ~X.columns.isin(self.columns)]\n",
    "        return pd.concat([X_notscaled, X_scaled], axis=1)[init_col_order]\n",
    "\n",
    "print('CustomScaler defined ‚úì')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to scale ‚Äî explicitly listed to exclude binary flags\n",
    "numerical_cols = ['CreditScore', 'Age', 'Tenure', 'Balance',\n",
    "                  'NumOfProducts', 'EstimatedSalary',\n",
    "                  'Satisfaction Score', 'Point Earned']\n",
    "\n",
    "churn_scaler = CustomScaler(columns=numerical_cols)\n",
    "churn_scaler.fit(X)          # Learn mean/std from all 10,000 real customers\n",
    "X = churn_scaler.transform(X)\n",
    "\n",
    "print('Scaling applied. Sanity checks:')\n",
    "print(f'  Balance mean  (should ‚âà 0) : {X[\"Balance\"].mean():.4f}')\n",
    "print(f'  Balance std   (should ‚âà 1) : {X[\"Balance\"].std():.4f}')\n",
    "print(f'  HasCrCard mean (unchanged)  : {X[\"HasCrCard\"].mean():.4f}  ‚Üê still a binary proportion')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fitted scaler ‚Äî must be the same object used during inference\n",
    "# CRITICAL: inference pipeline must call .transform(), never .fit_transform()\n",
    "with open('Scaler_file.pkl', 'wb') as f:\n",
    "    pickle.dump(churn_scaler, f)\n",
    "\n",
    "print('‚úÖ Scaler saved  ‚Üí  Scaler_file.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. One-Hot Encoding for Categorical Features\n",
    "\n",
    "ML models require numbers. We convert text categories to binary (0/1) columns.\n",
    "\n",
    "**`drop_first=True`** removes one category per feature to avoid the **dummy variable trap** (perfect multicollinearity):\n",
    "\n",
    "| Feature | Categories | After encoding |\n",
    "|---|---|---|\n",
    "| Geography | France, Germany, Spain | `Geography_Germany`, `Geography_Spain` (France = 0, 0) |\n",
    "| Gender | Female, Male | `Gender_Male` (Female = 0) |\n",
    "| Card Type | DIAMOND, GOLD, PLATINUM, SILVER | `_GOLD`, `_PLATINUM`, `_SILVER` (DIAMOND = 0,0,0) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['Geography', 'Gender', 'Card Type']\n",
    "\n",
    "print('Unique values before encoding:')\n",
    "for col in categorical_cols:\n",
    "    print(f'  {col}: {sorted(X[col].unique())}')\n",
    "\n",
    "data_dummies = pd.get_dummies(X, columns=categorical_cols, drop_first=True, dtype='int')\n",
    "\n",
    "print(f'\\nShape before encoding: {X.shape}')\n",
    "print(f'Shape after  encoding: {data_dummies.shape}')\n",
    "\n",
    "new_cols = [c for c in data_dummies.columns if c not in X.columns.tolist()]\n",
    "print('\\nNew dummy columns:', new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder to a fixed column sequence and re-attach the target\n",
    "# Fixed order is critical ‚Äî the model expects features in this exact arrangement\n",
    "FEATURE_COLUMNS = [\n",
    "    'HasCrCard', 'IsActiveMember', 'CreditScore', 'Age', 'Tenure',\n",
    "    'Balance', 'NumOfProducts', 'EstimatedSalary', 'Satisfaction Score',\n",
    "    'Point Earned', 'Geography_Germany', 'Geography_Spain', 'Gender_Male',\n",
    "    'Card Type_GOLD', 'Card Type_PLATINUM', 'Card Type_SILVER'\n",
    "]\n",
    "\n",
    "data_processed = pd.concat([data_dummies[FEATURE_COLUMNS], y], axis=1)\n",
    "\n",
    "print('Final processed data:')\n",
    "print(f'  Shape : {data_processed.shape}  (10,000 real customers ‚Äî NO synthetic samples)')\n",
    "print(f'  Target: {data_processed[\"Exited\"].value_counts().to_dict()}  (imbalanced ‚Äî SMOTE applied in N5)')\n",
    "data_processed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Class Imbalance ‚Äî Acknowledge, Do Not Fix Here\n",
    "\n",
    "The target is imbalanced (~80% stayed, ~20% churned). We visualise it here to document the state of the data, but **do not apply SMOTE yet**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = data_processed['Exited'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(['Stayed (0)', 'Churned (1)'], counts.values,\n",
    "       color=['#4C72B0', '#DD8452'], edgecolor='white', linewidth=1.5)\n",
    "for i, v in enumerate(counts.values):\n",
    "    ax.text(i, v + 50, f'{v:,}\\n({v/len(data_processed)*100:.1f}%)',\n",
    "            ha='center', fontweight='bold')\n",
    "ax.set_title('Class Imbalance in Saved Data\\n(SMOTE will be applied in N5 after train/test split)',\n",
    "             fontsize=11)\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_ylim(0, 9500)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('Imbalance ratio:', f\"{counts[0]/counts[1]:.1f} : 1  (stayed : churned)\")\n",
    "print()\n",
    "print('This imbalance is intentionally preserved in the saved file.')\n",
    "print('SMOTE will be applied ONLY to X_train in N5, after the train/test split.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed.to_csv('data_processed.csv', index=False)\n",
    "\n",
    "print('‚úÖ data_processed.csv saved')\n",
    "print(f'   Rows    : {data_processed.shape[0]:,}  (original, real customers only)')\n",
    "print(f'   Columns : {data_processed.shape[1]}  (16 features + 1 target)')\n",
    "print()\n",
    "print('Artifacts produced by N4:')\n",
    "print('  Scaler_file.pkl    ‚Üê fitted CustomScaler for inference pipeline')\n",
    "print('  data_processed.csv ‚Üê scaled + encoded, imbalanced ‚Äî ready for N5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Feature Engineering Summary\n",
    "\n",
    "| Step | Input | Output | Notes |\n",
    "|---|---|---|---|\n",
    "| Separate X / y | 14 cols | 13 features + 1 target | |\n",
    "| Standardise 8 numerical cols | Raw values | mean=0, std=1 | HasCrCard, IsActiveMember NOT scaled |\n",
    "| One-hot encode 3 categorical cols | Text | +6 binary columns | drop_first=True |\n",
    "| Reorder columns | Mixed order | 16 features in fixed order | |\n",
    "| **Save** | ‚Äî | **10,000 rows √ó 17 cols ‚Äî imbalanced** | |\n",
    "| SMOTE | ‚ùå Not here | ‚Äî | Applied in N5 after split |\n",
    "\n",
    "‚û°Ô∏è Continue to **N5_Model_Train_Test_Selection** where the correct train/test split and SMOTE pipeline is implemented."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

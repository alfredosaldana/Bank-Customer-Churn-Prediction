# ðŸ¦ Bank Customer Churn Prediction
### An End-to-End Machine Learning Pipeline â€” From Raw Data to Production-Ready Inference

[![Python](https://img.shields.io/badge/Python-3.12-blue?logo=python)](https://python.org)
[![sklearn](https://img.shields.io/badge/scikit--learn-1.x-orange?logo=scikit-learn)](https://scikit-learn.org)
[![License: MIT](https://img.shields.io/badge/License-MIT-green)](LICENSE)

---

## ðŸ“‹ Project Overview

This project builds a complete machine learning pipeline to predict whether a bank customer will **churn** (close their account or stop using services). Early identification of at-risk customers allows a bank to act proactively â€” offering personalised retention incentives before the customer leaves.

The project is structured as **7 sequential Jupyter notebooks** plus a **reusable Python inference module** and an **interactive Streamlit web app**, taking the dataset from raw CSV to a deploy-ready prediction system. Every design decision is documented and explained, making this project useful as both a portfolio piece and a learning resource.

---

## ðŸ—‚ï¸ Project Structure

```
bank-customer-churn/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ Customer-Churn-Records.csv          # Raw dataset (10,000 customers, 18 features)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ N1_data_upload.ipynb                # Data loading, shape, types, data dictionary
â”‚   â”œâ”€â”€ N2_ExploratoryDataAnalysis.ipynb    # EDA: distributions, correlations, leakage detection
â”‚   â”œâ”€â”€ N3_data_cleaning.ipynb              # Drop columns, outlier rationale, checkpoint save
â”‚   â”œâ”€â”€ N4_data_engineering.ipynb           # Scaling and one-hot encoding (no SMOTE here â€” see N5)
â”‚   â”œâ”€â”€ N5_Model_Train_Test_Selection.ipynb # Split â†’ SMOTE on train only â†’ train 6 models â†’ select best
â”‚   â”œâ”€â”€ N6_Model_Saving.ipynb               # Retrain on full real data, save production artifacts
â”‚   â””â”€â”€ N7_BankChurn_Module.ipynb           # Demo the production inference module
â”‚
â”œâ”€â”€ BankChurn_Module.py                     # Production inference class (import-ready)
â”œâ”€â”€ app.py                                  # Streamlit web app (batch + single customer GUI)
â”‚
â”œâ”€â”€ artifacts/                              # Generated by running the notebooks
â”‚   â”œâ”€â”€ model_file.pkl                      # Trained Random Forest model
â”‚   â””â”€â”€ Scaler_file.pkl                     # Fitted CustomScaler
â”‚
â””â”€â”€ README.md
```

---

## ðŸ“Š Dataset

**Source:** `Customer-Churn-Records.csv`  
**Shape:** 10,000 rows Ã— 18 columns  
**Target variable:** `Exited` (1 = churned, 0 = stayed)

| Column | Type | Description |
|---|---|---|
| `RowNumber` | int | Row index â€” dropped (no information) |
| `CustomerId` | int | Unique ID â€” dropped (no information) |
| `Surname` | str | Customer surname â€” dropped (no information + privacy) |
| `CreditScore` | int | Credit rating score (300â€“850) |
| `Geography` | str | Country: France, Germany, Spain |
| `Gender` | str | Male / Female |
| `Age` | int | Customer age in years |
| `Tenure` | int | Years as a bank customer |
| `Balance` | float | Account balance (â‚¬) |
| `NumOfProducts` | int | Number of bank products held (1â€“4) |
| `HasCrCard` | int | Holds credit card? 1=Yes, 0=No |
| `IsActiveMember` | int | Active member? 1=Yes, 0=No |
| `EstimatedSalary` | float | Estimated annual salary (â‚¬) |
| `Exited` | int | **TARGET**: Left the bank? 1=Yes, 0=No |
| `Complain` | int | Filed a complaint? 1=Yes, 0=No â€” **dropped (data leakage)** |
| `Satisfaction Score` | int | Customer satisfaction score (1â€“5) |
| `Card Type` | str | DIAMOND / GOLD / PLATINUM / SILVER |
| `Point Earned` | int | Loyalty points accumulated |

---

## ðŸ”‘ Key Findings

### 1. Critical Data Leakage: `Complain` must be dropped
The `Complain` column has a **0.99 Pearson correlation** with the target `Exited` â€” they are 99.7% identical row-for-row. Including `Complain` in the model would constitute **data leakage**: the model would learn a near-perfect shortcut that only exists in historical data and would be unavailable at prediction time in production.

### 2. Class Imbalance requires correction â€” applied correctly after the split
The dataset is 80% class 0 (stayed) / 20% class 1 (churned). We apply **SMOTE** (Synthetic Minority Over-sampling Technique) **only to the training set**, after the train/test split. This is critical: applying SMOTE before splitting would allow synthetic samples â€” generated from the full dataset â€” to contaminate the test set, producing inflated and misleading accuracy scores.

### 3. Key predictors of churn
Based on Random Forest feature importance:
- **Age** â€” older customers churn more
- **NumOfProducts** â€” customers with 3â€“4 products churn disproportionately (non-linear pattern)
- **IsActiveMember** â€” inactive members are significantly more likely to churn
- **Balance** â€” certain balance ranges correlate with higher churn
- **Geography** â€” German customers churn more relative to their share

### 4. Binary features must NOT be standardised
`HasCrCard` and `IsActiveMember` are binary (0/1). Standardising them would destroy their meaning. Our custom `CustomScaler` class scales only the eight continuous numerical features explicitly.

---

## ðŸ¤– Model Selection

Six classification algorithms were trained on the **SMOTE-balanced training set** and evaluated on the **real, imbalanced 20% test set** (no synthetic samples â€” unseen real customers only):

| Model | Accuracy | Precision | Recall | F1 | ROC-AUC |
|---|---|---|---|---|---|
| **Random Forest** â­ | highest | high | high | **highest** | **highest** |
| Gradient Boosting | competitive | competitive | competitive | 2nd | 2nd |
| Decision Tree | moderate | moderate | moderate | lower | lower |
| KNN | moderate | moderate | moderate | lower | â€” |
| SVC | moderate | moderate | moderate | lower | â€” |
| Logistic Regression | lowest | lowest | lowest | lowest | lowest |

> **Note:** Exact metric values depend on your run environment and sklearn version. Run N5 to see the actual scores on your machine. The ranking above is stable across runs.

**Random Forest** was selected based on highest F1 score and ROC-AUC, plus its robustness to outliers and built-in feature importance interpretability.

---

## âš™ï¸ Pipeline Steps

```
Raw CSV (10,000 Ã— 18)
    â”‚
    â”œâ”€â”€ [N2] EDA: distributions, correlations, leakage detection
    â”‚
    â”œâ”€â”€ [N3] Drop: RowNumber, CustomerId, Surname, Complain
    â”‚       â””â”€â”€ Output: 10,000 Ã— 14  (df_cleaned.csv)
    â”‚
    â”œâ”€â”€ [N4] Scale numerical features (CustomScaler â†’ Scaler_file.pkl)
    â”‚       Scale: CreditScore, Age, Tenure, Balance, NumOfProducts,
    â”‚              EstimatedSalary, Satisfaction Score, Point Earned
    â”‚       Skip:  HasCrCard, IsActiveMember (binary flags â€” must not be scaled)
    â”‚
    â”œâ”€â”€ [N4] One-Hot Encode: Geography, Gender, Card Type (drop_first=True)
    â”‚       â””â”€â”€ Output: 10,000 Ã— 17  (data_processed.csv â€” real, imbalanced)
    â”‚
    â”œâ”€â”€ [N5] Train/test split (80/20, stratified by target)
    â”‚       â””â”€â”€ X_train: 8,000 rows  |  X_test: 2,000 rows  (real customers only)
    â”‚
    â”œâ”€â”€ [N5] SMOTE applied to X_train / y_train ONLY
    â”‚       â””â”€â”€ X_train grows to ~16,000 rows (balanced 50/50)
    â”‚           X_test stays untouched â€” real, imbalanced, never seen by model
    â”‚
    â”œâ”€â”€ [N5] Train 6 models on SMOTE-balanced X_train
    â”‚       Evaluate all on real X_test â†’ select Random Forest
    â”‚
    â”œâ”€â”€ [N6] Retrain Random Forest on ALL 10,000 real customers (no SMOTE)
    â”‚       â””â”€â”€ model_file.pkl  (calibrated to real-world class distribution)
    â”‚
    â””â”€â”€ [N7] CustomerChurn class: load pkl files â†’ preprocess â†’ predict
```

---

## ðŸš€ Quick Start

### 1. Install Dependencies

```bash
# Core pipeline
pip install pandas numpy scikit-learn imbalanced-learn matplotlib seaborn

# Web app (optional)
pip install streamlit plotly
```

### 2. Run Notebooks in Order

Run notebooks N1 through N6 sequentially. Each notebook saves a checkpoint file used by the next.

| Notebook | Reads | Writes |
|---|---|---|
| N1 | `Customer-Churn-Records.csv` | â€” |
| N2 | `Customer-Churn-Records.csv` | â€” |
| N3 | `Customer-Churn-Records.csv` | `df_cleaned.csv` |
| N4 | `df_cleaned.csv` | `data_processed.csv`, `Scaler_file.pkl` |
| N5 | `data_processed.csv` | `prefinal_model.pkl` |
| N6 | `data_processed.csv` | `model_file.pkl` |
| N7 | `model_file.pkl`, `Scaler_file.pkl` | `churn_predictions_output.csv` |

### 3. Use the Inference Module

```python
from BankChurn_Module import CustomerChurn

# Load the trained model and scaler
churn_model = CustomerChurn(
    model_file  = 'artifacts/model_file.pkl',
    scaler_file = 'artifacts/Scaler_file.pkl'
)

# Preprocess raw customer data (CSV must match original schema)
churn_model.load_and_clean_data('new_customers.csv')

# Generate predictions
results = churn_model.predict_churn()

# results contains all original columns + 'Predicted_Exited'
print(results[['CustomerId', 'Surname', 'Predicted_Exited']].head())
```

### 4. Launch the Web App

```bash
streamlit run app.py
# Open http://localhost:8501 in your browser
# Upload model_file.pkl and Scaler_file.pkl via the sidebar
```

---

## ðŸ—ï¸ Design Decisions & Learning Points

### Why use a `CustomScaler` instead of sklearn's `StandardScaler` directly?
Standard `StandardScaler` scales ALL columns. Our `CustomScaler` wraps it to scale only the eight continuous numerical features, leaving binary flags unchanged, while remaining fully sklearn-compatible (fit/transform API, picklable).

### Why not scale `HasCrCard` and `IsActiveMember`?
These are binary indicator variables (0 or 1). StandardScaler would shift them to approximately (âˆ’0.86, 1.16) â€” destroying their semantic meaning. The model would no longer be able to treat them as clean on/off flags.

### Why is SMOTE applied in N5, not N4?
SMOTE generates synthetic minority-class samples by interpolating between real ones. If applied before the train/test split, synthetic samples generated from the full dataset appear in the test set â€” the model has effectively seen distorted versions of test samples during training. This inflates evaluation metrics and produces results that cannot be reproduced on genuinely new data. The correct position is **after** splitting: apply SMOTE only to `X_train`, leaving `X_test` as entirely real, unseen customers.

### Why does N6 train on the imbalanced full dataset (no SMOTE)?
The final production model should be calibrated to the **real-world class distribution** (~20% churn). If we SMOTE the final training data to 50/50, `predict_proba()` outputs will be systematically overestimated â€” the model thinks it lives in a world where half of customers churn, so it assigns inflated churn probabilities across the board. Retention teams would receive misleading risk scores and misallocate budget. Training on the real distribution keeps probability outputs meaningful.

### Why retrain on 100% of the data in N6?
N5 held out 20% to get an unbiased performance estimate. Once the model is selected and validated, that holdout data can be added back for final training â€” giving the production model more examples to learn from without any risk of circular evaluation.

### Why is the `Complain` column dangerous?
A customer complaint typically occurs *as part of* the churn event itself â€” it is not an independent early-warning signal. In a real deployment, you need to predict churn *before* it happens, so `Complain` would not yet be known. Using it gives the model future information unavailable at inference time â€” a classic **target leakage** scenario that produces near-perfect training accuracy and near-zero real-world value.

---

## ðŸ“¦ Requirements

```
# Data & ML
pandas>=1.5
numpy>=1.23
scikit-learn>=1.2
imbalanced-learn>=0.10

# Visualisation
matplotlib>=3.6
seaborn>=0.12

# Web app
streamlit>=1.30
plotly>=5.0
```

---

## ðŸŽ“ About This Project

This project was built as a portfolio piece demonstrating a methodologically rigorous end-to-end machine learning workflow: data exploration, principled cleaning, correct feature engineering, proper train/test separation, SMOTE applied without test-set contamination, model evaluation, and a production-ready inference class. Every step includes explanations written to be accessible to students and junior data scientists.

---

## ðŸ“„ License

MIT License â€” feel free to use, adapt, and share with attribution.

# üè¶ Bank Customer Churn Prediction

> An end-to-end machine learning pipeline to predict whether a bank customer is likely to leave ‚Äî packaged as a reusable, importable Python module.

---

## üìã Project Overview

Customer churn ‚Äî the phenomenon of customers discontinuing their relationship with a business ‚Äî is one of the most costly problems in retail banking. Acquiring a new customer costs **five to seven times more** than retaining an existing one, making early identification of at-risk customers a top strategic priority.

This project builds a complete ML pipeline using a dataset of **10,000 European bank customers**. The final deliverable is not just a notebook but a modular, importable Python class that can score new customer batches without retraining.

> ‚ö†Ô∏è **Central challenge:** The target variable is heavily imbalanced (~80% stayed, ~20% exited). A naive model that always predicts "stayed" would achieve 80% accuracy ‚Äî while being completely useless. This project addresses that problem using **SMOTE**, and evaluates models using the right metrics.

---

## üóÇÔ∏è Project Structure

```
bank-customer-churn/
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Customer-Churn-Records.csv    # Original Kaggle data (10,000 √ó 18)
‚îÇ   ‚îú‚îÄ‚îÄ processed/                         # Cleaned & engineered datasets
‚îÇ   ‚îî‚îÄ‚îÄ exports/
‚îÇ       ‚îî‚îÄ‚îÄ churn_tableau_export.csv       # For Tableau dashboard
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                             # 7 sequential notebooks
‚îÇ   ‚îú‚îÄ‚îÄ N1_data_upload.ipynb               # Load data, first look, data dictionary
‚îÇ   ‚îú‚îÄ‚îÄ N2_ExploratoryDataAnalysis.ipynb   # EDA: distributions, correlations, leakage
‚îÇ   ‚îú‚îÄ‚îÄ N3_data_cleaning.ipynb             # Drop identifiers, handle leakage, outliers
‚îÇ   ‚îú‚îÄ‚îÄ N4_data_engineering.ipynb          # Split ‚Üí encode ‚Üí scale (CustomScaler)
‚îÇ   ‚îú‚îÄ‚îÄ N5_Model_Train_Test_Selection.ipynb # SMOTE ‚Üí train 6 models ‚Üí compare metrics
‚îÇ   ‚îú‚îÄ‚îÄ N6_Model_Saving.ipynb              # Retrain on full data, save artifacts
‚îÇ   ‚îî‚îÄ‚îÄ N7_BankChurn_Module.ipynb          # Demo the production inference module
‚îÇ
‚îú‚îÄ‚îÄ artifacts/                             # Generated by running notebooks
‚îÇ   ‚îú‚îÄ‚îÄ model_file.pkl                     # Trained best model (Random Forest)
‚îÇ   ‚îî‚îÄ‚îÄ Scaler_file.pkl                    # Fitted CustomScaler
‚îÇ
‚îú‚îÄ‚îÄ BankChurn_Module.py                    # Production inference class (import-ready)
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üìä Dataset & Data Dictionary

**Source:** [Bank Customer Churn Dataset ‚Äî Kaggle](https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn)  
**Shape:** 10,000 rows √ó 18 columns  
**Target variable:** `Exited` (1 = churned, 0 = stayed)

| Column | Type | Description | Used in Model? |
|---|---|---|:---:|
| RowNumber | Integer | Sequential row index | ‚ùå Dropped |
| CustomerId | Integer | Unique customer identifier | ‚ùå Dropped |
| Surname | String | Customer last name | ‚ùå Dropped |
| CreditScore | Integer | Credit score (300‚Äì850). Higher = better creditworthiness | ‚úÖ Yes |
| Geography | String | Country: France, Germany, or Spain | ‚úÖ Encoded |
| Gender | String | Male or Female | ‚úÖ Encoded |
| Age | Integer | Customer age in years | ‚úÖ Yes |
| Tenure | Integer | Years as a bank customer (0‚Äì10) | ‚úÖ Yes |
| Balance | Float | Current account balance in Euros | ‚úÖ Yes |
| NumOfProducts | Integer | Number of bank products held (1‚Äì4) | ‚úÖ Yes |
| HasCrCard | Binary (0/1) | Has a credit card? 1 = Yes | ‚úÖ Yes |
| IsActiveMember | Binary (0/1) | Considered an active member? 1 = Yes | ‚úÖ Yes |
| EstimatedSalary | Float | Estimated annual salary in Euros | ‚úÖ Yes |
| **Exited** | Binary | **TARGET:** 1 = left the bank | üéØ Target |
| Complain | Binary | Filed a complaint? | ‚ö†Ô∏è DROPPED |
| Satisfaction Score | Integer | Customer satisfaction rating (1‚Äì5) | ‚úÖ Yes |
| Card Type | String | DIAMOND / GOLD / SILVER / PLATINUM | ‚úÖ Encoded |
| Point Earned | Integer | Reward points accumulated | ‚úÖ Yes |

> üö® **Critical Note:** `Complain` has a **0.99 Pearson correlation** with `Exited` ‚Äî they are 99.7% identical. Including it would constitute **target leakage**: the model would learn a near-perfect shortcut that only exists in historical data, not in real-world deployment.

---

## üî¨ Methodology

### The Class Imbalance Problem

```
Stayed (0):  7,963 customers  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  79.6%
Exited (1):  2,037 customers  ‚ñà‚ñà‚ñà‚ñà‚ñà                     20.4%
```

A model predicting "stayed" for everyone would be **79.6% accurate ‚Äî but useless**. This project addresses imbalance using **SMOTE** (Synthetic Minority Oversampling Technique).

### How SMOTE Works

Instead of simply duplicating existing minority samples, SMOTE creates new, **synthetic data points** by interpolating between a minority sample and its nearest neighbors in feature space. This forces the model to build a more robust decision boundary.

> ‚ö†Ô∏è **Critical rule:** SMOTE is applied **only to the training set**, after the train/test split. The test set remains as the original real distribution to provide an honest evaluation.

```python
from imblearn.over_sampling import SMOTE

sm = SMOTE(random_state=42)
X_train_res, y_train_res = sm.fit_resample(X_train, y_train)
# Before: {0: 6370, 1: 1630}
# After:  {0: 6370, 1: 6370}  ‚Üê balanced!
```

---

## üß™ Pipeline Steps

### Step 1 ‚Äî Data Upload & First Look (`N1`)

```python
df = pd.read_csv('data/raw/Customer-Churn-Records.csv')
print(f"Shape: {df.shape}")                       # (10000, 18)
print(df.isnull().sum())                           # No missing values
print(df['Exited'].value_counts(normalize=True))   # 80% stayed, 20% exited
```

### Step 2 ‚Äî Exploratory Data Analysis (`N2`)

Key questions answered:
- What is the overall churn rate? (**20.4%**)
- How does churn vary by geography, gender, age, and number of products?
- Are there correlations between numerical features?
- Is there data leakage? (**Yes ‚Äî `Complain` is 99% correlated with `Exited`**)

Key findings from EDA:
- **Churn rate by Geography:** Germany ~32% vs France/Spain ~16%
- **Age distribution:** Churners have a higher median age (40‚Äì60)
- **Correlation heatmap** revealing the `Complain` leakage

### Step 3 ‚Äî Data Cleaning (`N3`)

```python
# Drop identifier columns (no predictive value)
df_clean = df.drop(columns=['RowNumber', 'CustomerId', 'Surname'])

# Drop Complain (data leakage)
df_clean = df_clean.drop(columns=['Complain'])
```

### Step 4 ‚Äî Feature Engineering (`N4`)

> üîë **The most important rule: Split BEFORE fitting any transformers!**

```python
# Train/test split FIRST
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# One-hot encode categorical features
cat_cols = ['Geography', 'Gender', 'Card Type']
X_train = pd.get_dummies(X_train, columns=cat_cols, drop_first=True)
X_test  = pd.get_dummies(X_test,  columns=cat_cols, drop_first=True)
X_test  = X_test.reindex(columns=X_train.columns, fill_value=0)
```

**`CustomScaler` ‚Äî scales only numerical columns, not binary flags:**

```python
class CustomScaler:
    def __init__(self, num_cols):
        self.num_cols = num_cols
        self.scaler = StandardScaler()

    def fit(self, X):
        self.scaler.fit(X[self.num_cols])
        return self

    def transform(self, X):
        X_copy = X.copy()
        X_copy[self.num_cols] = self.scaler.transform(X_copy[self.num_cols])
        return X_copy
```

### Step 5 ‚Äî Model Training & Selection (`N5`)

Six algorithms were trained and compared:

| Model | Accuracy | Precision | Recall | F1 | ROC-AUC |
|---|:---:|:---:|:---:|:---:|:---:|
| **Random Forest** ‚≠ê | **0.93** | **0.93** | **0.93** | **0.93** | **0.98** |
| Gradient Boosting | 0.92 | 0.92 | 0.92 | 0.92 | 0.97 |
| Decision Tree | 0.88 | 0.88 | 0.88 | 0.88 | 0.88 |
| KNN | 0.87 | 0.87 | 0.87 | 0.87 | 0.95 |
| SVC | 0.84 | 0.84 | 0.84 | 0.84 | 0.92 |
| Logistic Regression | 0.80 | 0.80 | 0.80 | 0.80 | 0.87 |

**Why Random Forest won:**
- Highest F1 score and ROC-AUC
- Robust to outliers
- Built-in feature importance
- Handles non-linear relationships well

### Step 6 ‚Äî Model Saving (`N6`)

```python
import pickle

# Retrain on 100% of SMOTE-balanced data
best_model = RandomForestClassifier(n_estimators=100, random_state=42)
best_model.fit(X_train_res, y_train_res)

with open('artifacts/model_file.pkl', 'wb') as f:
    pickle.dump(best_model, f)

with open('artifacts/Scaler_file.pkl', 'wb') as f:
    pickle.dump(scaler, f)
```

### Step 7 ‚Äî Production Module Demo (`N7`)

```python
from BankChurn_Module import CustomerChurn

churn_predictor = CustomerChurn(
    model_file='artifacts/model_file.pkl',
    scaler_file='artifacts/Scaler_file.pkl'
)

churn_predictor.load_and_clean_data('new_customers.csv')
results = churn_predictor.predict_churn()
print(results[['CustomerId', 'Surname', 'Predicted_Exited']].head())
```

---

## üí° Key Findings & Business Insights

### üåç Finding 1 ‚Äî Geography matters significantly
German customers churn at nearly **twice the rate** of French or Spanish customers (~32% vs ~16%). This suggests competitive pressure or product-market fit issues specific to that region.

### üë§ Finding 2 ‚Äî Age is the strongest individual predictor
Customers aged **40‚Äì60** are substantially more likely to churn. This may indicate a "switching window" where mid-career customers re-evaluate financial relationships as their wealth grows.

### üì¶ Finding 3 ‚Äî Number of products is a double-edged sword
- Customers with **2 products**: lowest churn rate
- Customers with **3‚Äì4 products**: highest churn rate *(counterintuitive)*

This suggests forced bundling may cause dissatisfaction.

### üí∞ Finding 4 ‚Äî Account balance and churn have a bimodal relationship
Both **zero-balance customers** (dormant/disengaged) and **very high-balance customers** (shopping competitors) show elevated churn.

### ‚úÖ Finding 5 ‚Äî Active membership is strongly protective
Customers flagged as "active members" churn at roughly **half the rate** of inactive members.

### üèÜ Feature Importance (Random Forest)

| Rank | Feature |
|:---:|---|
| 1 | Age |
| 2 | NumOfProducts |
| 3 | IsActiveMember |
| 4 | Balance |
| 5 | Geography_Germany |

---

## üêç Using the `CustomerChurn` Module

```python
from BankChurn_Module import CustomerChurn

# Initialize
churn_predictor = CustomerChurn(
    model_file='artifacts/model_file.pkl',
    scaler_file='artifacts/Scaler_file.pkl'
)

# Load new data (must match original schema, minus Exited)
churn_predictor.load_and_clean_data('new_customers.csv')

# Get predictions
predictions = churn_predictor.predict_churn()
print(f"Predicted to churn: {predictions['Predicted_Exited'].sum()} of {len(predictions)} customers")
```

**The class automatically:**
- Drops non-predictive identifier columns (`RowNumber`, `CustomerId`, `Surname`)
- Drops `Complain` (leakage variable)
- Applies one-hot encoding to categorical fields
- Scales numerical columns using the saved scaler
- Returns original data with an added `Predicted_Exited` column

---

## üìä Tableau Dashboard ‚Äî "Bank Churn Command Center"

A business-facing interactive dashboard for exploring churn patterns without code.

| Sheet | Visualization | Key Insight |
|---|---|---|
| Sheet 1 | Churn Rate by Geography (Bar/Map) | Germany at ~32% vs France/Spain at ~16% |
| Sheet 2 | Age Distribution: Churners vs. Stayers | The 40‚Äì60 age risk window |
| Sheet 3 | Churn Rate by Number of Products | The 3‚Äì4 product anomaly |
| Sheet 4 | Feature Importance (Horizontal bar) | Top 15 features from Random Forest |
| Sheet 5 | Churn Probability Distribution | Customer risk segmentation |

**Export for Tableau:**
```python
tableau_df = df_clean.copy()
tableau_df['Churn_Probability'] = best_model.predict_proba(X)[:, 1]
tableau_df.to_csv('data/exports/churn_tableau_export.csv', index=False)
```

---

## üéØ Why Not Accuracy? ‚Äî Evaluation Metrics Explained

| Metric | Formula | Why It Matters Here |
|---|---|---|
| Precision | TP / (TP + FP) | Of all predicted churners, how many actually churned? |
| **Recall** | TP / (TP + FN) | Of all actual churners, how many did we catch? ‚Üê **priority** |
| F1-Score | Harmonic mean of P & R | Balance between precision and recall |
| ROC-AUC | Area under ROC curve | Overall separability ‚Äî model-threshold independent |

In this business context, **recall on the churn class is the priority metric**. A false negative (missed churner) is costlier than a false positive (unnecessary retention offer to a loyal customer).

---

## ‚öôÔ∏è Installation & Quick Start

```bash
# 1. Clone the repository
git clone https://github.com/alfredosaldana/P01_BankCustomerChurn_Module.git
cd bank-customer-churn

# 2. Install dependencies
pip install -r requirements.txt

# 3. Download the dataset from Kaggle and place in data/raw/
#    https://www.kaggle.com/datasets/radheshyamkollipara/bank-customer-churn

# 4. Run notebooks in order: N1 ‚Üí N2 ‚Üí N3 ‚Üí N4 ‚Üí N5 ‚Üí N6 ‚Üí N7

# 5. Use the inference module on new data
python -c "
from BankChurn_Module import CustomerChurn
model = CustomerChurn('artifacts/model_file.pkl', 'artifacts/Scaler_file.pkl')
model.load_and_clean_data('new_customers.csv')
print(model.predict_churn().head())
"
```

**`requirements.txt`:**
```
pandas>=1.5.0
numpy>=1.23.0
scikit-learn>=1.1.0
imbalanced-learn>=0.10.0
matplotlib>=3.6.0
seaborn>=0.12.0
plotly>=5.10.0
shap>=0.41.0
joblib>=1.2.0
```

---

## ‚ö†Ô∏è Limitations & Caveats

- **Educational dataset:** This Kaggle dataset is simulated ‚Äî findings may not generalize to real bank data without revalidation.
- **`Complain` leakage:** In a production system, the timing of complaints would determine if it's a valid predictor. Here, it's dropped.
- **Static snapshot:** No transaction history or behavioral time-series features. Real churn models benefit from trend features (e.g., declining balance over 6 months).
- **SMOTE caveat:** Synthetic samples are mathematically plausible but not real customers. Aggressive oversampling can cause overfitting.
- **Threshold selection:** The default 0.5 threshold is not necessarily optimal. Tune based on business costs (false negative vs. false positive).
- **Geography scope:** Only France, Germany, Spain. The model cannot be applied elsewhere without retraining.
- **Binary features not scaled:** `HasCrCard` and `IsActiveMember` remain 0/1 ‚Äî intentional, but must be handled consistently.

---

## üöÄ Next Steps & Future Work

- **SHAP explainability:** Add SHAP values to explain individual predictions ‚Äî critical for regulatory and stakeholder acceptance
- **Hyperparameter tuning:** Apply Optuna Bayesian optimization to the final model
- **REST API deployment:** Wrap `CustomerChurn` in FastAPI for real-time scoring from a CRM
- **Model monitoring:** Implement data drift detection with `evidently` to alert when retraining is needed
- **Time-series features:** Enrich with rolling statistics if transaction-level data becomes available
- **Cost-sensitive learning:** Explore `class_weight` and custom cost matrices as SMOTE alternatives
- **Animated churn trends:** Use Plotly to show how churn evolves over time (if multi-year data)

---

## üéì Skills Demonstrated

| Component | Skill Demonstrated |
|---|---|
| EDA notebooks | Data intuition, visual communication |
| SMOTE + imbalanced learning | Handling real-world messy data |
| Train/test split discipline | ML best practices, preventing leakage |
| Multi-model comparison | Statistical reasoning, model evaluation |
| `CustomScaler` class | Software engineering for data science |
| Tableau dashboard | Business communication, BI tools |
| This README | Technical writing and documentation |

---

## üìÑ License

MIT License ‚Äî feel free to use, adapt, and share with attribution.  
Dataset sourced from Kaggle under its respective terms of use.

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè¶ Bank Customer Churn Prediction\n",
    "## Notebook 5 ‚Äî Model Training & Selection\n",
    "\n",
    "**Goal:** Train and compare six classification models and select the best one.\n",
    "\n",
    "### The Correct SMOTE Position ‚Äî Why It Matters\n",
    "\n",
    "SMOTE creates *synthetic* minority-class samples by interpolating between existing real samples. If applied **before** the train/test split, some synthetic samples end up in the test set. Those samples were generated from the full dataset ‚Äî including the test portion ‚Äî so the model has effectively seen distorted versions of the test data during training. The result: **artificially inflated accuracy** that does not reflect real-world performance.\n",
    "\n",
    "The fix is a strict three-step sequence:\n",
    "\n",
    "```\n",
    "Step 1:  Split the REAL, imbalanced data  ‚Üí  X_train, X_test, y_train, y_test\n",
    "Step 2:  Apply SMOTE to X_train / y_train ONLY\n",
    "           - X_test stays untouched (real customers, original class ratio)\n",
    "Step 3:  Train models on SMOTE-balanced training set\n",
    "           - Evaluate on the real, untouched test set\n",
    "```\n",
    "\n",
    "This guarantees that model performance is measured on data the model has **never seen in any form**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model   import LogisticRegression\n",
    "from sklearn                import svm\n",
    "from sklearn.neighbors      import KNeighborsClassifier\n",
    "from sklearn.tree           import DecisionTreeClassifier\n",
    "from sklearn.ensemble       import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics        import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# Load the imbalanced (real) processed data from N4\n",
    "data = pd.read_csv('data_processed.csv')\n",
    "print(f'Data loaded: {data.shape}')\n",
    "print(f'Class balance (original): {data[\"Exited\"].value_counts().to_dict()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train / Test Split ‚Äî BEFORE Any Resampling\n",
    "\n",
    "`stratify=y` ensures the 80/20 class ratio is preserved in both splits.  \n",
    "Without stratification, random sampling could by chance put most churners in one set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Exited', axis=1)\n",
    "y = data['Exited']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=y          # ‚Üê preserves the original class ratio in both splits\n",
    ")\n",
    "\n",
    "print('Split complete (stratified):')\n",
    "print(f'  X_train : {X_train.shape[0]:,} rows  ‚Üí  class ratio: {y_train.value_counts().to_dict()}')\n",
    "print(f'  X_test  : {X_test.shape[0]:,}  rows  ‚Üí  class ratio: {y_test.value_counts().to_dict()}')\n",
    "print()\n",
    "print(f'  Test set churn rate : {y_test.mean()*100:.1f}%  (matches original dataset ‚âà 20%)')\n",
    "print()\n",
    "print('‚ö†Ô∏è  X_test will NOT be touched again until final evaluation.')\n",
    "print('    It contains only real customers ‚Äî zero synthetic samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Apply SMOTE ‚Äî to X_train Only\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique)** generates synthetic minority-class samples by interpolating between existing ones ‚Äî not just duplicating rows.\n",
    "\n",
    "Applying it only to `X_train` / `y_train` means:\n",
    "- The model trains on a **balanced** representation of both classes.\n",
    "- The test set remains **100% real data** at the original 80/20 ratio.\n",
    "- Evaluation metrics reflect true real-world performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "print('SMOTE applied to training set only:')\n",
    "print(f'  Before ‚Üí {y_train.value_counts().to_dict()}')\n",
    "print(f'  After  ‚Üí {y_train_sm.value_counts().to_dict()}')\n",
    "print()\n",
    "print(f'  X_train rows : {X_train.shape[0]:,} ‚Üí {X_train_sm.shape[0]:,}  (+{X_train_sm.shape[0]-X_train.shape[0]:,} synthetic)')\n",
    "print(f'  X_test  rows : {X_test.shape[0]:,}  ‚Üê unchanged (all real)')\n",
    "\n",
    "# Visualise the train vs test class balance\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "configs = [\n",
    "    (y_train,    'Training Set (before SMOTE)', '#4C72B0'),\n",
    "    (y_train_sm, 'Training Set (after SMOTE)',  '#3FB950'),\n",
    "    (y_test,     'Test Set ‚Äî real data only',   '#DD8452'),\n",
    "]\n",
    "for ax, (series, title, color) in zip(axes, configs):\n",
    "    vc = series.value_counts()\n",
    "    ax.bar(['Stayed', 'Churned'], vc.values, color=[color, '#E05C55'], alpha=0.85, edgecolor='white')\n",
    "    for i, v in enumerate(vc.values):\n",
    "        ax.text(i, v + 30, f'{v:,}', ha='center', fontweight='bold', fontsize=9)\n",
    "    ax.set_title(title, fontsize=10)\n",
    "    ax.set_ylabel('Count')\n",
    "plt.suptitle('Class Balance: Training vs Test Sets', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding the Evaluation Metrics\n",
    "\n",
    "For churn prediction, different errors have different business costs:\n",
    "\n",
    "- **False Negative** = missed churner ‚Üí customer leaves undetected ‚Üí **high cost** (lost revenue)\n",
    "- **False Positive** = wrongly flagged non-churner ‚Üí wasted retention spend ‚Üí **low-medium cost**\n",
    "\n",
    "Therefore **Recall** and **F1** are more important than raw accuracy.\n",
    "\n",
    "```\n",
    "Precision = TP / (TP + FP)  ‚Üê of all predicted churners, how many actually churned?\n",
    "Recall    = TP / (TP + FN)  ‚Üê of all actual churners, how many did we catch?\n",
    "F1        = 2 √ó (Precision √ó Recall) / (Precision + Recall)\n",
    "ROC-AUC   = ranking quality across all probability thresholds (1.0 = perfect)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train All Six Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All models trained on SMOTE-balanced training set\n",
    "# All models evaluated on the untouched, imbalanced, real test set\n",
    "models = {\n",
    "    'Logistic Regression' : LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'SVC'                 : svm.SVC(probability=True, random_state=42),\n",
    "    'KNN'                 : KNeighborsClassifier(),\n",
    "    'Decision Tree'       : DecisionTreeClassifier(random_state=42),\n",
    "    'Random Forest'       : RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting'   : GradientBoostingClassifier(random_state=42),\n",
    "}\n",
    "\n",
    "results = []\n",
    "print('Training on SMOTE-balanced X_train, evaluating on REAL X_test...\\n')\n",
    "\n",
    "for name, model in models.items():\n",
    "    # Train on SMOTE data\n",
    "    model.fit(X_train_sm, y_train_sm)\n",
    "\n",
    "    # Predict on REAL test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "\n",
    "    results.append({\n",
    "        'Model'    : name,\n",
    "        'Accuracy' : accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall'   : recall_score(y_test, y_pred),\n",
    "        'F1'       : f1_score(y_test, y_pred),\n",
    "        'ROC-AUC'  : roc_auc_score(y_test, y_prob) if y_prob is not None else np.nan,\n",
    "    })\n",
    "    r = results[-1]\n",
    "    print(f'  {name:22s}  Acc={r[\"Accuracy\"]:.3f}  F1={r[\"F1\"]:.3f}  AUC={r[\"ROC-AUC\"]:.3f}')\n",
    "\n",
    "print()\n",
    "print('Note: accuracy is no longer ~100% ‚Äî these are realistic scores on real, imbalanced test data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).set_index('Model').sort_values('F1', ascending=False)\n",
    "\n",
    "print('Performance on REAL test set (sorted by F1 Score):')\n",
    "print(results_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1', 'ROC-AUC']\n",
    "x       = np.arange(len(results_df))\n",
    "width   = 0.15\n",
    "colors  = ['#4C72B0', '#DD8452', '#55A868', '#C44E52', '#8172B2']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "for i, (metric, color) in enumerate(zip(metrics, colors)):\n",
    "    ax.bar(x + i * width, results_df[metric], width,\n",
    "           label=metric, color=color, alpha=0.85)\n",
    "\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(results_df.index, rotation=20, ha='right', fontsize=10)\n",
    "ax.set_ylim(0, 1.05)\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Comparison ‚Äî Evaluated on Real, Imbalanced Test Set', fontsize=13, fontweight='bold')\n",
    "ax.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Deep Dive: Random Forest (Selected Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC = models['Random Forest']\n",
    "y_pred_RFC = RFC.predict(X_test)\n",
    "\n",
    "print('=== Random Forest ‚Äî Classification Report on REAL test set ===')\n",
    "print(classification_report(y_test, y_pred_RFC,\n",
    "                             target_names=['Stayed (0)', 'Churned (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_RFC)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, linewidths=1,\n",
    "            xticklabels=['Predicted: Stay', 'Predicted: Churn'],\n",
    "            yticklabels=['Actual: Stay', 'Actual: Churn'])\n",
    "ax.set_title('Random Forest ‚Äî Confusion Matrix\\n(evaluated on real test customers)', fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'True Negatives  (correctly predicted Stay)  : {tn:,}')\n",
    "print(f'False Positives (wrongly predicted Churn)   : {fp:,}')\n",
    "print(f'False Negatives (missed actual Churners)    : {fn:,}')\n",
    "print(f'True Positives  (correctly predicted Churn) : {tp:,}')\n",
    "print()\n",
    "print(f'Of the {fn+tp} customers who actually churned, we caught {tp} ({tp/(fn+tp)*100:.1f}%).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importances = pd.Series(RFC.feature_importances_, index=X.columns).sort_values(ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "bars = ax.barh(importances.index, importances.values,\n",
    "               color=plt.cm.RdYlGn(importances.values / importances.max()),\n",
    "               edgecolor='white')\n",
    "ax.set_title('Random Forest ‚Äî Feature Importance', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Mean Decrease in Impurity (Gini)')\n",
    "for bar, val in zip(bars, importances.values):\n",
    "    ax.text(val + 0.001, bar.get_y() + bar.get_height()/2,\n",
    "            f'{val:.3f}', va='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Selected Model (Pre-Final)\n",
    "\n",
    "This model was trained on the SMOTE-balanced training split. It is the \"evaluation\" model.  \n",
    "In N6, we retrain on all 10,000 real customers (no SMOTE needed at that point ‚Äî see N6 for rationale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(RFC, 'prefinal_model.pkl')\n",
    "print('‚úÖ Pre-final model saved  ‚Üí  prefinal_model.pkl')\n",
    "print()\n",
    "print('Selected: Random Forest')\n",
    "print('Reasons :')\n",
    "print('  1. Highest F1 and ROC-AUC among all six models')\n",
    "print('  2. Tree-based ‚Üí robust to outliers, no assumption of linearity')\n",
    "print('  3. Built-in feature importance for interpretability')\n",
    "print('  4. Ensemble of 100 trees ‚Üí less prone to overfitting than single Decision Tree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Model Selection Summary\n",
    "\n",
    "| Step | Detail |\n",
    "|---|---|\n",
    "| Split | 80% train / 20% test, stratified |\n",
    "| Resampling | SMOTE applied to **X_train only** |\n",
    "| Training data | SMOTE-balanced (‚âà 16,000 rows) |\n",
    "| Test data | **Real, imbalanced** (2,000 rows, 80/20 ratio) |\n",
    "| Selected model | Random Forest |\n",
    "| Accuracy reported | **Realistic** ‚Äî no synthetic contamination of test set |\n",
    "\n",
    "‚û°Ô∏è Continue to **N6_Model_Saving** to train the final production model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
